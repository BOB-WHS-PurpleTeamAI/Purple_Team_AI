{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IOnRdRn5iin",
        "outputId": "7b4fd5c3-1dc2-4b28-927b-316aef547803"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q938UAc4J67q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import pandas as pd\n",
        "import sentencepiece as spm\n",
        "from sentencepiece import SentencePieceProcessor\n",
        "import os\n",
        "import json\n",
        "from logging import getLogger"
      ],
      "metadata": {
        "id": "m0HrjyPo5nE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습된 vocab에서 토큰 임베딩 구하기\n",
        "<br>\n",
        "vocab모델 로딩"
      ],
      "metadata": {
        "id": "q4P0DFwOQ6Kl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_file = \"pytest.model\"\n",
        "#vocab = spm.SentencePieceProcessor()\n",
        "#vocab.load(vocab_file)\n",
        "\n",
        "logger = getLogger()"
      ],
      "metadata": {
        "id": "PWxuTy_YQ5Xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "입력값을 임베딩 해주기"
      ],
      "metadata": {
        "id": "Rew7td0xfawt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class tokenizer:\n",
        "  def __init__(self):\n",
        "      super().__init__()\n",
        "      model_path = \"pytest.model\"\n",
        "      assert os.path.isfile(model_path), model_path\n",
        "      self.sp_model = SentencePieceProcessor(model_file=model_path)\n",
        "      logger.info(f\"Reloaded SentencePiece model from {model_path}\")\n",
        "  def encode(lines):\n",
        "    inputs = []\n",
        "    for line in lines:\n",
        "      #pieces = vocab.encode_as_pieces(line)\n",
        "      ids = sp_model.encode_as_ids(line)\n",
        "      inputs.append(torch.tensor(ids))\n",
        "    inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
        "    return inputs\n",
        "  def decode(lines):\n",
        "    input =[]\n",
        "    for line in lines:\n",
        "      line=line.tolist()\n",
        "      txt=sp_model.decode(line)\n",
        "      input.append(txt)\n",
        "    return input"
      ],
      "metadata": {
        "id": "uTYn1ngJW9n6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(lines):\n",
        "  inputs = []\n",
        "  for line in lines:\n",
        "    #pieces = vocab.encode_as_pieces(line)\n",
        "    ids = vocab.encode_as_ids(line)\n",
        "    inputs.append(torch.tensor(ids))\n",
        "  inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=0)\n",
        "  return inputs\n",
        "def decode(lines):\n",
        "  input =[]\n",
        "  for line in lines:\n",
        "    line=line.tolist()\n",
        "    txt=vocab.decode(line)\n",
        "    input.append(txt)\n",
        "    #vocab.decode(line)\n",
        "  return input\n",
        "#encode([\"print(lamda x: for i in x)\",\"for in range(len a)\"])\n",
        "print(encode([\"print(lamda x: for i in x)\",\"for in range(len(a))\"]))\n",
        "print(decode(encode([\"print(lamda x: for i in x)\",\"for in range(len(a))\"])))\n",
        "#decode(encode(\"print(lamda x: for i in x)\"))"
      ],
      "metadata": {
        "id": "CpATWruWbVmQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2270f7a-5d27-4769-936c-735623a0645b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  32, 7934,  128, 7931,  298,  130, 7941,   65,   17,   28,  130, 7935],\n",
            "        [  65,   28,  143, 7934,  103, 7934, 7925,   72,    0,    0,    0,    0]])\n",
            "['print(lamda x: for i in x)', 'for in range(len(a))']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "트렌스포머 설정 로딩"
      ],
      "metadata": {
        "id": "fKReA5yjfgsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" configuration json을 읽어들이는 class \"\"\"\n",
        "class Config(dict):\n",
        "    __getattr__ = dict.__getitem__\n",
        "    __setattr__ = dict.__setitem__\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, file):\n",
        "        with open(file, 'r') as f:\n",
        "            config = json.loads(f.read())\n",
        "            return Config(config)\n",
        "mconfig=Config.load(\"config.json\")"
      ],
      "metadata": {
        "id": "nRl4-H69fkdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습 데이터로 부터 베치 만들기"
      ],
      "metadata": {
        "id": "zqu0QK6y4jGo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "rmsnorm"
      ],
      "metadata": {
        "id": "j0XiM-bNOKUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, layer_shape, eps=1e-8, bias=False):\n",
        "        super(RMSNorm, self).__init__()\n",
        "        self.register_parameter(\"scale\", nn.Parameter(torch.ones(layer_shape)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        assumes shape is (batch, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        # frob norm is not the same as RMS. RMS = 1/sqrt(N) * frob norm\n",
        "        ff_rms = torch.linalg.norm(x, dim=(1,2)) * x[0].numel() ** -.5\n",
        "        raw = x / ff_rms.unsqueeze(-1).unsqueeze(-1)\n",
        "        return self.scale[:x.shape[1], :].unsqueeze(0) * raw\n"
      ],
      "metadata": {
        "id": "lYzDS3zN7QLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "swiglu"
      ],
      "metadata": {
        "id": "FmodQQWsOee7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SwiGLU(nn.Module):\n",
        "    \"\"\"\n",
        "    Swish-Gated Linear Unit\n",
        "    https://arxiv.org/pdf/2002.05202v1.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self, size):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.linear_gate = nn.Linear(size, size)\n",
        "        self.linear = nn.Linear(size, size)\n",
        "        self.beta = torch.randn(1, requires_grad=True)\n",
        "\n",
        "        self.beta = nn.Parameter(torch.ones(1))\n",
        "        self.register_parameter(\"beta\", self.beta)\n",
        "\n",
        "    def forward(self, x):\n",
        "        swish_gate = self.linear_gate(x) * torch.sigmoid(self.beta * self.linear_gate(x))\n",
        "        out = swish_gate * self.linear(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "1A_kYO3IOV9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "rope"
      ],
      "metadata": {
        "id": "A32urhYrOjH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_rotary_matrix(context_window, embedding_dim):\n",
        "    R = torch.zeros((context_window, embedding_dim, embedding_dim), requires_grad=False)\n",
        "    for position in range(context_window):\n",
        "        for i in range(embedding_dim//2):\n",
        "            theta = 10000. ** (-2.*(i - 1) / embedding_dim)\n",
        "            m_theta = position * theta\n",
        "            R[position, 2*i,2*i] = np.cos(m_theta)\n",
        "            R[position, 2*i,2*i+1] = - np.sin(m_theta)\n",
        "            R[position, 2*i+1,2*i] = np.sin(m_theta)\n",
        "            R[position, 2*i+1,2*i+1] = np.cos(m_theta)\n",
        "    return R\n",
        "class RoPEMaskedAttentionHead(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.w_q = nn.Linear(config['d_model'], config['d_model'], bias=False)\n",
        "        self.w_k = nn.Linear(config['d_model'], config['d_model'], bias=False)\n",
        "        self.w_v = nn.Linear(config['d_model'], config['d_model'], bias=False)\n",
        "\n",
        "        self.R = get_rotary_matrix(config['context_window'], config['d_model'])\n",
        "\n",
        "    def get_rotary_matrix(context_window, embedding_dim):\n",
        "        R = torch.zeros((context_window, embedding_dim, embedding_dim), requires_grad=False)\n",
        "        for position in range(context_window):\n",
        "            for i in range(embedding_dim//2):\n",
        "                theta = 10000. ** (-2.*(i - 1) / embedding_dim)\n",
        "                m_theta = position * theta\n",
        "                R[position, 2*i,2*i] = np.cos(m_theta)\n",
        "                R[position, 2*i,2*i+1] = - np.sin(m_theta)\n",
        "                R[position, 2*i+1,2*i] = np.sin(m_theta)\n",
        "                R[position, 2*i+1,2*i+1] = np.cos(m_theta)\n",
        "        return R\n",
        "\n",
        "    def forward(self, x, return_attn_weights=False):\n",
        "        b,m,d = x.shape\n",
        "\n",
        "        q = self.w_q(x)\n",
        "        k = self.w_k(x)\n",
        "        v = self.w_v(x)\n",
        "\n",
        "        q_rotated = (torch.bmm(q.transpose(0,1), self.R[:m])).transpose(0,1)\n",
        "        k_rotated = (torch.bmm(k.transpose(0,1), self.R[:m])).transpose(0,1)\n",
        "\n",
        "        activations = F.scaled_dot_product_attention(\n",
        "            q_rotated,k_rotated,v,dropout_p =.1, is_causal=True\n",
        "        )\n",
        "\n",
        "        if return_attn_weights:\n",
        "            attn_mask = torch.tril(torch.ones((m,m)), diagonal=0)\n",
        "            attn_weights = torch.bmm(q_rotated, k_rotated.transpose(1,2)) / np.sqrt(d) + attn_mask\n",
        "            attn_weights = F.softmax(attn_weights, dim=-1)\n",
        "            return activations, attn_weights\n",
        "        return activations"
      ],
      "metadata": {
        "id": "Ha0cUfepeP_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class myblock(nn.module):\n",
        "  def __init__(self,config):\n",
        "      super().__init__()\n",
        "      self.config=config\n",
        "      self.rms = RMSNorm((config['context_window'], config['d_model']))\n",
        "      self.attention = RoPEMaskedMultiheadAttention(config)\n",
        "      self.feedforward = nn.Sequential(\n",
        "            nn.Linear(config['d_model'], config['d_model']),\n",
        "            SwiGLU(config['d_model']),\n",
        "        )\n",
        "  def forward(self, x):\n",
        "        x = self.rms(x) # rms pre-normalization\n",
        "        x = x + self.attention(x)\n",
        "\n",
        "        x = self.rms(x) # rms pre-normalization\n",
        "        x = x + self.feedforward(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "WkV19SYdR-57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습실행코드 생성"
      ],
      "metadata": {
        "id": "w_KZ1uDvVsRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train(model, optimizer, scheduler=None, config=MASTER_CONFIG, print_logs=False):\n",
        "    losses = []\n",
        "    start_time = time.time()\n",
        "    for epoch in range(config['epochs']):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        xs, ys = get_batches(dataset, 'train', config['batch_size'], config['context_window'])\n",
        "        logits, loss = model(xs, targets=ys)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "        if epoch % config['log_interval'] == 0:\n",
        "            batch_time = time.time() - start_time\n",
        "            x = evaluate_loss(model)\n",
        "            losses += [x]\n",
        "            if print_logs:\n",
        "                print(f\"Epoch {epoch} | val loss {x['val']:.3f} | Time {batch_time:.3f} | ETA in seconds {batch_time * (config['epochs'] - epoch)/config['log_interval'] :.3f}\")\n",
        "            start_time = time.time()\n",
        "\n",
        "            if scheduler:\n",
        "                print(\"lr: \", scheduler.get_lr())\n",
        "\n",
        "    print(\"validation loss: \", losses[-1]['val'])\n",
        "    return pd.DataFrame(losses).plot()"
      ],
      "metadata": {
        "id": "uTUOrVpUVoU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습한 토크나이저를 이용해서 만든 학습용 파일에서 크기 맞춰서 입력"
      ],
      "metadata": {
        "id": "q_xgtF54J9fJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(split, batch_size, config=MASTER_CONFIG):\n",
        "  if split == 'val':\n",
        "    val=open(\"val.json\",'r')\n",
        "    batch_data = val\n",
        "  elif split == 'test':\n",
        "    test=open(\"test.json\",'r')\n",
        "    batch_data = test\n",
        "  else:\n",
        "    train=open(\"train.json\",'r')\n",
        "    batch_data = train\n",
        "  d=json.load(batch_data)\n",
        "  id_dict = {item[\"id\"]: item for item in d}\n",
        "  max_id = max(id_dict.keys()) if id_dict else None\n",
        "  rand= random.randint(0, max_id-batch_size)\n",
        "  x=[]\n",
        "  y=[]\n",
        "  for _ in range(rand,rand+batch_size):\n",
        "    ans,q=searchbyid(_,id_dict)\n",
        "    x.append(ans)\n",
        "    y.append(q)\n",
        "    rand=rand+1\n",
        "  x = torch.nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=0)\n",
        "  y = torch.nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=0)\n",
        "  return x,y\n"
      ],
      "metadata": {
        "id": "f0fWY35KJ8ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "배치를 가져와서 모델 평가하는 코드"
      ],
      "metadata": {
        "id": "2Mp6Wdq-KTFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()  # don't compute gradients for this function\n",
        "def evaluate_loss(model, config=MASTER_CONFIG):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = []\n",
        "        for _ in range(10):\n",
        "            xb, yb = get_batches(split, config['batch_size'])\n",
        "            _, loss = model(xb, yb)\n",
        "            losses.append(loss.item())\n",
        "        out[split] = np.mean(losses)\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "fMCpBRLzKOLJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}