{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeF555PLg2Of",
        "outputId": "0a91d3cb-2285-4c32-cdde-5cb97d0d3d21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from sentencepiece import SentencePieceProcessor\n",
        "import json\n",
        "import random\n",
        "import copy\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "egKqpmASg9pE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spm=SentencePieceProcessor(model_file=\"pytest.model\")"
      ],
      "metadata": {
        "id": "UpvlKb5VhIAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_dec=512 #디코더 모델의 크기\n",
        "def makebatch(datafile,outfile,sid):\n",
        "  qs=[]\n",
        "  ans=[]\n",
        "  lines = open(datafile, 'r').readlines()\n",
        "  id=sid\n",
        "  with open(outfile, \"w\") as f:\n",
        "    for line in lines:\n",
        "      ans=spm.encode_as_ids(line)\n",
        "      l=len(ans)\n",
        "      if(l>n_dec-2):\n",
        "        ans=ans[:n_dec-2]\n",
        "      ans.insert(0,2) #vocab 상 2=bos 3= eos 6=mask\n",
        "      ans.append(3)\n",
        "      ran=random.randrange(1,l+2)\n",
        "      qs=copy.deepcopy(ans)\n",
        "      qs[ran]=6\n",
        "      datablock={\"ids\":id,\"anser\":ans,\"question\":qs}\n",
        "      f.write(json.dumps(datablock))\n",
        "      f.write(\"\\n\")\n",
        "      id+=1\n"
      ],
      "metadata": {
        "id": "N9YGjsXfhLwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_dec=512 #디코더 모델의 크기\n",
        "def makebatch(datafile, trainfile, testfile, valfile):\n",
        "    qs = []\n",
        "    ans = []\n",
        "    lines = open(datafile, 'r').readlines()\n",
        "    train_data, test_data = train_test_split(lines, test_size=0.3, random_state=42)\n",
        "    test_data, val_data = train_test_split(test_data, test_size=0.2, random_state=42)\n",
        "    def write_to_file(file, data):\n",
        "        id=0\n",
        "        with open(file, \"w\") as f:\n",
        "            f.write(\"[\")\n",
        "            for i, line in enumerate(data):\n",
        "                ans = spm.encode_as_ids(line)\n",
        "                l = len(ans)\n",
        "                if (l > n_dec - 2):\n",
        "                    ans = ans[:n_dec - 2]\n",
        "                ans.insert(0, 2)  # vocab 상 2=bos 3= eos 6=mask\n",
        "                ans.append(3)\n",
        "                ran = random.randrange(1, l + 2)\n",
        "                qs = copy.deepcopy(ans)\n",
        "                qs[ran] = 6\n",
        "                datablock = {\"id\": id, \"anser\": ans, \"question\": qs}\n",
        "                f.write(json.dumps(datablock))\n",
        "                if i == len(data) - 1:\n",
        "                  f.write(\"\\n\")\n",
        "                else:\n",
        "                  f.write(\",\\n\")\n",
        "                id += 1\n",
        "            f.write(\"]\")\n",
        "    write_to_file(trainfile, train_data)\n",
        "    write_to_file(testfile, test_data)\n",
        "    write_to_file(valfile, val_data)"
      ],
      "metadata": {
        "id": "Ueiv4rDwX7wP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "makebatch(\"input.txt\",\"train.json\",\"test.json\",\"val.json\")"
      ],
      "metadata": {
        "id": "26z99HjN4sRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines = open(\"input.txt\", 'r').readlines()\n",
        "train_data, test_data = train_test_split(lines, test_size=0.3, random_state=42)\n",
        "test_data, val_data = train_test_split(test_data, test_size=0.2, random_state=42)\n",
        "if train_data==\"\\t\":\n",
        "  print(11)\n",
        "print(ord('\\n'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j07zPZGGbG71",
        "outputId": "4a7906e1-2fc8-48fa-9e3b-7c2e48642bdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AvDRzrCAeRch"
      }
    }
  ]
}