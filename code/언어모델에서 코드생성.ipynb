{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGzf-oPinH-N",
        "outputId": "6035ce6c-b0a9-4d79-816f-acdbc167de29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from sentencepiece import SentencePieceProcessor\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import pandas as pd\n",
        "import json\n",
        "import random\n",
        "import copy\n",
        "from collections import OrderedDict\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "Ptt1qUS9nQBT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "토크나이져 모델 로드"
      ],
      "metadata": {
        "id": "W-Ae6_XInlre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spm=SentencePieceProcessor(model_file=\"sql2.model\")"
      ],
      "metadata": {
        "id": "uTSqFUmAnkzO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "하이퍼 퍼라미터 선언"
      ],
      "metadata": {
        "id": "ymCik-CLntpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MASTER_CONFIG = {\n",
        "    \"vocab_size\": spm.vocab_size(), #토크나이져 크기\n",
        "    'd_model': 128,\n",
        "    'batch_size':256,\n",
        "    'epochs':10,\n",
        "    'context_window':64,\n",
        "    'log_interval': 10,\n",
        "    'n_heads': 8, #헤드\n",
        "    'n_layers': 4,\n",
        "}"
      ],
      "metadata": {
        "id": "AtO-Z2XGn1Yu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MASTER_CONFIG.update({\n",
        "    'd_model': 128,\n",
        "    'batch_size':256,\n",
        "    'epochs':10,\n",
        "    'context_window':8,\n",
        "    'log_interval': 1,\n",
        "    'n_heads': 8,\n",
        "    'n_layers': 4,\n",
        "    's_id':0,\n",
        "})"
      ],
      "metadata": {
        "id": "NhGxPW3koSCi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습,검증용 데이터 토큰화 해서 생성"
      ],
      "metadata": {
        "id": "NRCuMZkTpZEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_dec=MASTER_CONFIG['context_window']\n",
        "def makebatch(datafile, trainfile, testfile, valfile):\n",
        "    qs = []\n",
        "    ans = []\n",
        "    lines = open(datafile, 'r').readlines()\n",
        "    train_data, test_data = train_test_split(lines, test_size=0.3, random_state=42)\n",
        "    test_data, val_data = train_test_split(test_data, test_size=0.2, random_state=42)\n",
        "    def write_to_file(file, data):\n",
        "        id=0\n",
        "        with open(file, \"w\") as f:\n",
        "            f.write(\"[\")\n",
        "            for i, line in enumerate(data):\n",
        "                ans = spm.encode_as_ids(line)\n",
        "                l = len(ans)\n",
        "                if (l > n_dec - 2):\n",
        "                    ans = ans[:n_dec - 2]\n",
        "                    l = len(ans)\n",
        "                ans.insert(0, 2)  # vocab 상 2=bos 3= eos 6=mask\n",
        "                ans.append(3)\n",
        "                ran = random.randrange(1, l + 2)\n",
        "                qs = copy.deepcopy(ans)\n",
        "                qs[ran] = 6\n",
        "                datablock = {\"id\": id, \"anser\": ans, \"question\": qs}\n",
        "                f.write(json.dumps(datablock))\n",
        "                if i == len(data) - 1:\n",
        "                  f.write(\"\\n\")\n",
        "                else:\n",
        "                  f.write(\",\\n\")\n",
        "                id += 1\n",
        "            f.write(\"]\")\n",
        "    write_to_file(trainfile, train_data)\n",
        "    write_to_file(testfile, test_data)\n",
        "    write_to_file(valfile, val_data)"
      ],
      "metadata": {
        "id": "o1PoKbrDnx6S"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "makebatch(\"Kaggle_SQL_Data.txt\",\"train.json\",\"test.json\",\"val.json\")"
      ],
      "metadata": {
        "id": "AULYw_Rmnomt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def searchbyid(id,id_dict):\n",
        "  if id in id_dict:\n",
        "    result = id_dict[id]\n",
        "    ans=result.get('anser')\n",
        "    q=result.get('question')\n",
        "    ans=torch.tensor(ans,dtype=torch.long)\n",
        "    q=torch.tensor(q,dtype=torch.long)\n",
        "    return ans,q\n"
      ],
      "metadata": {
        "id": "Wqe73IrWqE36"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(split, batch_size, config=MASTER_CONFIG):\n",
        "  if split == 'val':\n",
        "    val=open(\"val.json\",'r')\n",
        "    batch_data = val\n",
        "  elif split == 'test':\n",
        "    test=open(\"test.json\",'r')\n",
        "    batch_data = test\n",
        "  else:\n",
        "    train=open(\"train.json\",'r')\n",
        "    batch_data = train\n",
        "  d=json.load(batch_data)\n",
        "  id_dict = {item[\"id\"]: item for item in d}\n",
        "  max_id = max(id_dict.keys()) if id_dict else None\n",
        "  rand= random.randint(0, max_id-batch_size)\n",
        "  x=[]\n",
        "  y=[]\n",
        "  for _ in range(rand,rand+batch_size):\n",
        "    ans,q=searchbyid(_,id_dict)\n",
        "    size_of_ans=ans.size()[0]\n",
        "    if size_of_ans>MASTER_CONFIG['context_window']:\n",
        "      ans=ans[:MASTER_CONFIG['context_window']]\n",
        "      q=q[:MASTER_CONFIG['context_window']]\n",
        "    x.append(ans)\n",
        "    y.append(q)\n",
        "  size_of_x = x[0].size()[0]\n",
        "  size_of_y = y[0].size()[0]\n",
        "  x[0]=F.pad(x[0], (0,MASTER_CONFIG['context_window']-int(size_of_x)), mode='constant', value=0)\n",
        "  y[0]=F.pad(y[0], (0,MASTER_CONFIG['context_window']-int(size_of_y)), mode='constant', value=0)\n",
        "  x = torch.nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=0)\n",
        "  y = torch.nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=0)\n",
        "  return x,y\n"
      ],
      "metadata": {
        "id": "pZ_3Y4MHOY6-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "시퀀셜 하게 모든데이터 순회 하고싶은데 oom나옴"
      ],
      "metadata": {
        "id": "QYJPaIzMelm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(split,startid,batch_size, config=MASTER_CONFIG):\n",
        "  if split == 'val':\n",
        "    val=open(\"val.json\",'r')\n",
        "    batch_data = val\n",
        "  elif split == 'test':\n",
        "    test=open(\"test.json\",'r')\n",
        "    batch_data = test\n",
        "  else:\n",
        "    train=open(\"train.json\",'r')\n",
        "    batch_data = train\n",
        "  d=json.load(batch_data)\n",
        "  id_dict = {item[\"id\"]: item for item in d}\n",
        "  max_id = max(id_dict.keys()) if id_dict else None\n",
        "  x=[]\n",
        "  y=[]\n",
        "  for _ in range(startid,startid+max_id):\n",
        "    ans,q=searchbyid(_,id_dict)\n",
        "    size_of_ans=ans.size()[0]\n",
        "    if size_of_ans>MASTER_CONFIG['context_window']:\n",
        "      ans=ans[:MASTER_CONFIG['context_window']]\n",
        "      q=q[:MASTER_CONFIG['context_window']]\n",
        "    x.append(ans)\n",
        "    y.append(q)\n",
        "\n",
        "  size_of_x = x[0].size()[0]\n",
        "  size_of_y = y[0].size()[0]\n",
        "  x[0]=F.pad(x[0], (0,MASTER_CONFIG['context_window']-int(size_of_x)), mode='constant', value=0)\n",
        "  y[0]=F.pad(y[0], (0,MASTER_CONFIG['context_window']-int(size_of_y)), mode='constant', value=0)\n",
        "  x = torch.nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=0)\n",
        "  y = torch.nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=0)\n",
        "  return x,y"
      ],
      "metadata": {
        "id": "jlh9_M0TB4Ws"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "llama 모델"
      ],
      "metadata": {
        "id": "FfVt6ihVwwss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()  # don't compute gradients for this function\n",
        "def evaluate_loss(model, config=MASTER_CONFIG):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = []\n",
        "        for _ in range(10):\n",
        "            xb, yb = get_batches(split, config['batch_size'])\n",
        "            _, loss = model(xb, yb)\n",
        "            losses.append(loss.item())\n",
        "        out[split] = np.mean(losses)\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "VUOLBNUq7HRQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델선언 cpu ver"
      ],
      "metadata": {
        "id": "H6nMwUsJeXfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, scheduler=None, config=MASTER_CONFIG, print_logs=False):\n",
        "    losses = []\n",
        "    start_time = time.time()\n",
        "    for epoch in tqdm(range(config['epochs'])):\n",
        "        optimizer.zero_grad()\n",
        "        #sid=MASTER_CONFIG['s_id']\n",
        "        #xs, ys = get_batches( 'train',sid,MASTER_CONFIG['batch_size'], config=MASTER_CONFIG)\n",
        "        xs, ys = get_batches( 'train',MASTER_CONFIG['batch_size'], config=MASTER_CONFIG)\n",
        "        logits, loss = model(xs, targets=ys.long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #sid=sid+MASTER_CONFIG['batch_size']\n",
        "\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "        if epoch % config['log_interval'] == 0:\n",
        "            batch_time = time.time() - start_time\n",
        "            x = evaluate_loss(model)\n",
        "            losses += [x]\n",
        "            if print_logs:\n",
        "                print(f\"Epoch {epoch} | val loss {x['val']:.3f} | Time {batch_time:.3f} | ETA in seconds {batch_time * (config['epochs'] - epoch)/config['log_interval'] :.3f}\")\n",
        "            start_time = time.time()\n",
        "\n",
        "            if scheduler:\n",
        "                print(\"lr: \", scheduler.get_lr())\n",
        "\n",
        "    print(\"validation loss: \", losses[-1]['val'])\n",
        "    return pd.DataFrame(losses).plot()\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, layer_shape, eps=1e-8, bias=False):\n",
        "        super(RMSNorm, self).__init__()\n",
        "        self.register_parameter(\"scale\", nn.Parameter(torch.ones(layer_shape)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        assumes shape is (batch, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        # frob norm is not the same as RMS. RMS = 1/sqrt(N) * frob norm\n",
        "        ff_rms = torch.linalg.norm(x, dim=(1,2)) * x[0].numel() ** -.5\n",
        "        raw = x / ff_rms.unsqueeze(-1).unsqueeze(-1)\n",
        "        return self.scale[:x.shape[1], :].unsqueeze(0) * raw\n",
        "def get_rotary_matrix(context_window, embedding_dim):\n",
        "    R = torch.zeros((context_window, embedding_dim, embedding_dim), requires_grad=False)\n",
        "    for position in range(context_window):\n",
        "        for i in range(embedding_dim//2):\n",
        "            theta = 10000. ** (-2.*(i - 1) / embedding_dim)\n",
        "            m_theta = position * theta\n",
        "            R[position, 2*i,2*i] = np.cos(m_theta)\n",
        "            R[position, 2*i,2*i+1] = - np.sin(m_theta)\n",
        "            R[position, 2*i+1,2*i] = np.sin(m_theta)\n",
        "            R[position, 2*i+1,2*i+1] = np.cos(m_theta)\n",
        "    return R\n",
        "class RoPEMaskedAttentionHead(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.w_q = nn.Linear(config['d_model'], config['d_model'], bias=False)\n",
        "        self.w_k = nn.Linear(config['d_model'], config['d_model'], bias=False)\n",
        "        self.w_v = nn.Linear(config['d_model'], config['d_model'], bias=False)\n",
        "\n",
        "        self.R = get_rotary_matrix(config['context_window'], config['d_model'])\n",
        "\n",
        "    def forward(self, x, return_attn_weights=False):\n",
        "        b,m,d = x.shape\n",
        "\n",
        "        q = self.w_q(x)\n",
        "        k = self.w_k(x)\n",
        "        v = self.w_v(x)\n",
        "\n",
        "        q_rotated = (torch.bmm(q.transpose(0,1), self.R[:m])).transpose(0,1)\n",
        "        k_rotated = (torch.bmm(k.transpose(0,1), self.R[:m])).transpose(0,1)\n",
        "\n",
        "        activations = F.scaled_dot_product_attention(\n",
        "            q_rotated,k_rotated,v,dropout_p =.1, is_causal=True\n",
        "        )\n",
        "\n",
        "        if return_attn_weights:\n",
        "            attn_mask = torch.tril(torch.ones((m,m)), diagonal=0)\n",
        "            attn_weights = torch.bmm(q_rotated, k_rotated.transpose(1,2)) / np.sqrt(d) + attn_mask\n",
        "            attn_weights = F.softmax(attn_weights, dim=-1)\n",
        "            return activations, attn_weights\n",
        "        return activations\n",
        "\n",
        "class RoPEMaskedMultiheadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.heads = nn.ModuleList([\n",
        "            RoPEMaskedAttentionHead(config) for _ in range(config['n_heads'])\n",
        "        ])\n",
        "        self.linear = nn.Linear(config['n_heads'] * config['d_model'], config['d_model'])\n",
        "        self.dropout = nn.Dropout(.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        heads = [h(x) for h in self.heads]\n",
        "        x = torch.cat(heads, dim=-1)\n",
        "        x = self.linear(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    \"\"\"\n",
        "    Swish-Gated Linear Unit\n",
        "    https://arxiv.org/pdf/2002.05202v1.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self, size):\n",
        "        super().__init__()\n",
        "        self.config = MASTER_CONFIG\n",
        "        self.linear_gate = nn.Linear(size, size)\n",
        "        self.linear = nn.Linear(size, size)\n",
        "        self.beta = torch.randn(1, requires_grad=True)\n",
        "\n",
        "        self.beta = nn.Parameter(torch.ones(1))\n",
        "        self.register_parameter(\"beta\", self.beta)\n",
        "\n",
        "    def forward(self, x):\n",
        "        swish_gate = self.linear_gate(x) * torch.sigmoid(self.beta * self.linear_gate(x))\n",
        "        out = swish_gate * self.linear(x)\n",
        "        return out\n",
        "\n",
        "class LlamaBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.rms = RMSNorm((config['context_window'], config['d_model']))\n",
        "\n",
        "        self.attention = RoPEMaskedMultiheadAttention(config)\n",
        "        self.feedforward = nn.Sequential(\n",
        "            nn.Linear(config['d_model'], config['d_model']),\n",
        "            SwiGLU(config['d_model']),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.rms(x) # rms pre-normalization\n",
        "        x = x + self.attention(x)\n",
        "\n",
        "        x = self.rms(x) # rms pre-normalization\n",
        "        x = x + self.feedforward(x)\n",
        "        return x\n",
        "class Llama(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.embeddings = nn.Embedding(config['vocab_size'], config['d_model'])\n",
        "        self.llama_blocks = nn.Sequential(\n",
        "            OrderedDict([(f\"llama_{i}\", LlamaBlock(config)) for i in range(config['n_layers'])])\n",
        "        )\n",
        "\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(config['d_model'], config['d_model']),\n",
        "            SwiGLU(config['d_model']),\n",
        "            nn.Linear(config['d_model'], config['vocab_size']),\n",
        "        )\n",
        "\n",
        "        print(\"model params:\", sum([m.numel() for m in self.parameters()]))\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        x = self.embeddings(idx)\n",
        "        x = self.llama_blocks(x)\n",
        "        logits = self.ffn(x)\n",
        "\n",
        "        if targets is None:\n",
        "            return logits\n",
        "\n",
        "        else:\n",
        "            loss = F.cross_entropy(logits.view(-1, self.config['vocab_size']), targets.view(-1))\n",
        "            return logits, loss"
      ],
      "metadata": {
        "id": "v1byzyjjrGKj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llama = Llama(MASTER_CONFIG)\n",
        "\n",
        "# Load the saved weights into the model\n",
        "path_to_saved_weights = 'lm5.pth'\n",
        "llama.load_state_dict(torch.load(path_to_saved_weights,map_location=\"cpu\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_XV_6spcG5F",
        "outputId": "9c376f24-8e01-4a53-d98b-3491528ca746"
      },
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model params: 2865244\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 222
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, config=MASTER_CONFIG, max_new_tokens=10):\n",
        "    idx = torch.tensor([[2]]).long()\n",
        "    #idx.to(\"cuda\")\n",
        "    for _ in range(max_new_tokens):\n",
        "        # call the model\n",
        "        logits = model(idx[:, -config['context_window']:])\n",
        "        last_time_step_logits = logits[\n",
        "            :, -1, :\n",
        "        ]  # all the batches (1), last time step, all the logits\n",
        "        #last_time_step_logits=remove_first_n_elements(last_time_step_logits,5)\n",
        "        p = F.softmax(last_time_step_logits, dim=-1)  # softmax to get probabilities\n",
        "        idx_next = torch.multinomial(\n",
        "            p, num_samples=1\n",
        "        )  # sample from the distribution to get the next token\n",
        "        idx = torch.cat([idx, idx_next], dim=-1)  # append to the sequence\n",
        "    return spm.decode(idx.tolist())"
      ],
      "metadata": {
        "id": "32GHy1mTVgiM"
      },
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_first_n_elements(tensor, n):\n",
        "    # 텐서의 크기 가져오기\n",
        "    original_size = tensor.size()\n",
        "    # 제거할 요소 수가 총 요소 수보다 많은 경우\n",
        "    if n >= original_size[1]:\n",
        "        return None  # 예외처리 또는 원하는 대응 방법을 선택할 수 있습니다.\n",
        "\n",
        "    # 앞의 n개 요소를 제거하여 원하는 모양의 텐서로 변환\n",
        "    tensers=[]\n",
        "    for _ in range(original_size[0]):\n",
        "      new_tensor = tensor[_][n:]\n",
        "      new_size = original_size[1]\n",
        "      new_size -= n\n",
        "      new_tensor = new_tensor.view(new_size)\n",
        "      tensers.append(new_tensor)\n",
        "\n",
        "    return torch.stack(tensers)"
      ],
      "metadata": {
        "id": "-CDVKGJDWIZw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(llama)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nF133L4HVhwh",
        "outputId": "825e5159-1475-4173-e982-e260aa12ffc8"
      },
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['SELECT * FROM FETCH FIRST FETCH 5 )']"
            ]
          },
          "metadata": {},
          "execution_count": 251
        }
      ]
    }
  ]
}